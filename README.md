# Speaker Verification System for Regional Languages (Hindi & Kannada)

A deep learning-based speaker verification system implementing **ECAPA-TDNN** architecture for regional language (Hindi and Kannada) speaker recognition, achieving **7.88% EER** with balanced data distribution.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Achievements](#key-achievements)
- [Features](#features)
- [Dataset](#dataset)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Training Strategy](#training-strategy)
- [Evaluation Metrics](#evaluation-metrics)
- [Results](#results)
- [Demo](#demo)
- [References](#references)

## ğŸ¯ Overview

This project implements a state-of-the-art speaker verification system designed for regional Indian languages, specifically Hindi and Kannada. The system:

- **ECAPA-TDNN Architecture**: Fine-tuned from VoxCeleb2 pretrained model
- **Balanced Data Distribution**: Per-speaker 80/20 train/test split
- **Comprehensive Evaluation**: EER, accuracy, ROC curves, t-SNE visualizations
- **Robust Training**: Two-stage fine-tuning with data augmentation
- **Production-Ready**: Interactive demo with batch verification support

## ğŸ† Key Achievements

- âœ… **7.88% Test EER** with balanced data distribution
- âœ… **88.7% Accuracy** on Hindi/Kannada speaker verification
- âœ… **68.4% Relative Improvement** over imbalanced baseline (24.90% â†’ 7.88%)
- âœ… **100% Demo Accuracy** on genuine verification (10/10) and impostor rejection (4/4)
- âœ… **351 Speakers** with 17,330 audio files
- âœ… **Two-Stage Fine-Tuning** with encoder freezing strategy

## âœ¨ Features

### Data Processing

- âœ… Balanced per-speaker 80/20 split (13,725 train / 3,605 test files)
- âœ… Automatic audio preprocessing (8kHz mono)
- âœ… Variable-length audio handling (2-10 seconds)
- âœ… Speaker-based data organization (351 speakers)

### Data Augmentation

- âœ… Speed perturbation (0.95x, 1.0x, 1.05x)
- âœ… Additive white noise (SNR 0-15 dB)
- âœ… Reverberation simulation

### Model Training

- âœ… Pretrained ECAPA-TDNN from VoxCeleb2
- âœ… Two-stage fine-tuning (frozen encoder â†’ full training)
- âœ… AAM-Softmax loss with margin=0.2, scale=30
- âœ… Adam optimizer with lr=0.0001
- âœ… Automatic checkpoint saving (best validation accuracy)
- âœ… Training history visualization

### Evaluation

- âœ… Equal Error Rate (EER) computation
- âœ… Accuracy metrics on test set
- âœ… ROC curves with visualization
- âœ… Score distribution plots (genuine vs impostor)
- âœ… t-SNE embedding space visualization
- âœ… Batch verification for multiple samples

## ğŸ“Š Dataset

### Dataset Characteristics

- **Languages**: Hindi and Kannada
- **Total Speakers**: 351
- **Total Audio Files**: 17,330
- **Files per Speaker**: ~49 (average)
- **Data Split**:
  - **Balanced**: 80% train (13,725 files) / 20% test (3,605 files) per speaker
  - **Imbalanced Baseline**: 3 train files per speaker from Train/ folder only
- **Audio Format**:
  - Sample rate: 8 kHz (telephone quality)
  - Duration: Variable (~2-10 seconds)
  - Channels: Mono
  - Bit depth: 16-bit
  - Format: WAV

### Dataset Structure

```
data/
â”œâ”€â”€ Train/
â”‚   â”œâ”€â”€ 1034/
â”‚   â”‚   â”œâ”€â”€ 1034_trn_vp_a_1.wav
â”‚   â”‚   â”œâ”€â”€ 1034_trn_vp_a_2.wav
â”‚   â”‚   â””â”€â”€ ... (~49 files)
â”‚   â”œâ”€â”€ 1037/
â”‚   â””â”€â”€ ... (351 speakers)
â””â”€â”€ Test/
    â”œâ”€â”€ 1034/
    â”‚   â”œâ”€â”€ 1034_tst_vp_a_001.wav
    â”‚   â”œâ”€â”€ 1034_tst_vp_a_002.wav
    â”‚   â””â”€â”€ ... (~49 files)
    â”œâ”€â”€ 1037/
    â””â”€â”€ ... (351 speakers)
```

**Note**: With balanced splitting enabled in config, both Train/ and Test/ folders are combined and re-split 80/20 per speaker for better performance.

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA 11.8+ (for GPU support) or Google Colab with Tesla T4
- 8GB+ RAM recommended
- 5GB+ free disk space (excluding dataset)

### Step 1: Clone Repository

```bash
git clone https://github.com/Shrusti-04/Speaker-Verification.git
cd Speaker-Verification
```

### Step 2: Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n speaker_verification python=3.9
conda activate speaker_verification

# Or using venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows
```

### Step 3: Install Dependencies

```bash
# Install all dependencies
pip install -r requirements.txt
```

**Key Dependencies:**

- `torch>=2.0.0` - PyTorch deep learning framework
- `torchaudio>=2.0.0` - Audio processing
- `speechbrain>=0.5.0` - ECAPA-TDNN pretrained models
- `scikit-learn>=1.3.0` - Evaluation metrics
- `matplotlib>=3.7.0` - Visualization
- `pyyaml>=6.0` - Configuration management

### Step 4: Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import speechbrain; print('SpeechBrain OK')"
python -c "import torchaudio; print(f'TorchAudio: {torchaudio.__version__}')"
```

## ğŸ“ Project Structure

```
Speaker-Verification/
â”œâ”€â”€ config/                          # Configuration files
â”‚   â”œâ”€â”€ ecapa_balanced_config.yaml  # Balanced split (7.88% EER)
â”‚   â””â”€â”€ ecapa_config.yaml           # Imbalanced baseline (24.90% EER)
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ dataset.py                  # Dataset with balanced splitting
â”‚   â”œâ”€â”€ augmentation.py             # Audio augmentation
â”‚   â”œâ”€â”€ evaluation.py               # EER and metrics computation
â”‚   â”œâ”€â”€ verification.py             # Cosine similarity verification
â”‚   â”œâ”€â”€ visualization.py            # Plotting utilities
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ ecapa_tdnn.py          # ECAPA-TDNN wrapper + AAMSoftmax
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ data/                           # Dataset (not pushed to GitHub)
â”‚   â”œâ”€â”€ Train/                     # 351 speakers with audio files
â”‚   â””â”€â”€ Test/                      # 351 speakers with audio files
â”œâ”€â”€ checkpoints/                    # Trained models (not pushed)
â”‚   â”œâ”€â”€ ecapa/                     # Imbalanced baseline
â”‚   â””â”€â”€ ecapa_balanced/            # Best model (7.88% EER)
â”œâ”€â”€ logs/                           # Training logs (not pushed)
â”œâ”€â”€ results/                        # Evaluation outputs
â”‚   â”œâ”€â”€ ecapa_results.txt          # Metrics
â”‚   â”œâ”€â”€ ecapa_roc_curve.png        # ROC curve
â”‚   â”œâ”€â”€ ecapa_score_distribution.png
â”‚   â””â”€â”€ ecapa_tsne.png
â”œâ”€â”€ paper/                          # Documentation
â”‚   â”œâ”€â”€ EXPERIMENTAL_SETUP.md
â”‚   â”œâ”€â”€ TRAINING_LOG.md
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md
â”‚   â””â”€â”€ figures/
â”œâ”€â”€ train.py                        # Main training script
â”œâ”€â”€ evaluate.py                     # Evaluation script
â”œâ”€â”€ demo.py                         # Interactive verification demo
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ FILE_INDEX.md                   # Complete file documentation
â””â”€â”€ README.md                       # This file
```

**Note**: Large files (data, checkpoints, logs) are excluded via `.gitignore`.

## ğŸ“ Usage

### 1. Prepare Dataset

Organize your audio files in the `data/` directory:

```
data/
â”œâ”€â”€ Train/
â”‚   â”œâ”€â”€ speaker_id_1/
â”‚   â”‚   â””â”€â”€ *.wav files
â”‚   â””â”€â”€ speaker_id_2/
â”‚       â””â”€â”€ *.wav files
â””â”€â”€ Test/
    â”œâ”€â”€ speaker_id_1/
    â””â”€â”€ speaker_id_2/
```

### 2. Configure Training

Edit `config/ecapa_balanced_config.yaml` to customize:

- `use_combined_dataset: true` - Enable balanced 80/20 split
- `train_split: 0.8` - Train/test ratio per speaker
- `batch_size: 32` - Batch size (adjust for GPU memory)
- `learning_rate: 0.0001` - Learning rate
- `max_epochs: 15` - Number of training epochs
- `freeze_encoder_epochs: 5` - Epochs to freeze encoder

### 3. Train Model

```bash
# Train with balanced data (recommended)
python train.py --config config/ecapa_balanced_config.yaml

# Train with imbalanced data (baseline)
python train.py --config config/ecapa_config.yaml
```

**Training Progress:**

- Epoch 1-5: Encoder frozen, only classifier trains
- Epoch 6-15: Full model training with unfrozen encoder
- Best model saved based on validation accuracy
- Training history plots saved to `logs/`

**Expected Training Time:**

- Google Colab Tesla T4: ~4-5 hours for 15 epochs
- Local GPU (RTX 3080): ~2-3 hours

### 4. Evaluate Model

```bash
# Evaluate best model
python evaluate.py --config config/ecapa_balanced_config.yaml

# Evaluate specific checkpoint
python evaluate.py \
    --config config/ecapa_balanced_config.yaml \
    --checkpoint checkpoints/ecapa_balanced/best_model.pt
```

**Evaluation Outputs:**

- `results/ecapa_results.txt` - EER, accuracy metrics
- `results/ecapa_roc_curve.png` - ROC curve visualization
- `results/ecapa_score_distribution.png` - Score histograms
- `results/ecapa_tsne.png` - Embedding space visualization

### 5. Interactive Demo

```bash
# Single verification
python demo.py single \
    --model checkpoints/ecapa_balanced/best_model.pt \
    --enroll data/Train/1034/1034_trn_vp_a_1.wav \
    --test data/Test/1034/1034_tst_vp_a_001.wav

# Batch verification (enroll multiple + test multiple)
python demo.py batch \
    --model checkpoints/ecapa_balanced/best_model.pt \
    --enroll-dir data/Train/1034 \
    --test-dir data/Test/1034
```

**Demo Output Example:**

```
Enrollment: Processing 3 audio files...
Enrollment embedding created successfully

Testing against 10 audio files...
[âœ“] 1034_tst_vp_a_001.wav - MATCH (similarity: 0.7101)
[âœ“] 1034_tst_vp_a_002.wav - MATCH (similarity: 0.6845)
...

Results at threshold 0.50:
  Genuine: 10/10 correct (100.0%)
  Impostor: 4/4 correct (100.0%)
```

## ğŸ—ï¸ Model Architecture

### ECAPA-TDNN (Emphasized Channel Attention, Propagation and Aggregation in TDNN)

- **Source**: [SpeechBrain](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)
- **Pretrained on**: VoxCeleb2 (English speakers)
- **Embedding Dimension**: 192-D
- **Input**: Raw waveform at 8 kHz
- **Output**: 192-dimensional speaker embedding

**Architecture Components:**

- **SE-Res2Block**: Squeeze-and-Excitation with channel attention
- **Multi-layer Feature Aggregation**: Combines features from multiple layers
- **Attentive Statistical Pooling**: Weighted pooling across time
- **AAM-Softmax Loss**: Additive Angular Margin for better discrimination
  - Margin: 0.2
  - Scale: 30

**Model Size**: ~6.19M parameters

## ğŸ”¬ Training Strategy

### Two-Stage Fine-Tuning

**Stage 1: Frozen Encoder (Epochs 1-5)**

- Encoder weights frozen (pretrained from VoxCeleb2)
- Only classifier layer trains
- Adapts to 351 speakers
- Faster convergence

**Stage 2: Full Training (Epochs 6-15)**

- All layers unfrozen
- End-to-end fine-tuning
- Adapts to Hindi/Kannada audio characteristics
- Achieves final performance

### Data Distribution Strategy

**Balanced Split (Recommended):**

- Combines Train/ and Test/ folders
- Splits 80/20 per speaker
- Result: 13,725 train / 3,605 test files
- **Achieved 7.88% EER**

**Imbalanced Baseline:**

- Uses Train/ folder only (3 files/speaker)
- Result: 1,053 train / 8,775 test files
- Achieved 24.90% EER (poor performance)

### Data Augmentation

Applied randomly during training:

- **Speed Perturbation**: 0.95x, 1.0x, 1.05x (33% each)
- **Additive Noise**: White noise, SNR 0-15 dB
- **Reverberation**: Simulates room acoustics

### Hyperparameters

- **Optimizer**: Adam
- **Learning Rate**: 0.0001 (constant)
- **Batch Size**: 32
- **Max Epochs**: 15
- **Loss Function**: AAM-Softmax (margin=0.2, scale=30)
- **Hardware**: Google Colab Tesla T4 GPU
- **Training Time**: ~4-5 hours

## ğŸ“ˆ Evaluation Metrics

### Equal Error Rate (EER)

The threshold where False Acceptance Rate (FAR) equals False Rejection Rate (FRR).

- **Lower is better**
- Our Result: **7.88% EER**

### Accuracy

Percentage of correct verification decisions (genuine acceptance + impostor rejection).

- Our Result: **88.7% accuracy**

### Cosine Similarity Scoring

Measures similarity between speaker embeddings:

- Range: [-1, 1]
- Higher values indicate same speaker
- Threshold: 0.50 (optimized for EER)

### Visualization Outputs

- **ROC Curves**: True Positive Rate vs False Positive Rate
- **Score Distributions**: Genuine vs impostor score histograms
- **t-SNE Plots**: 2D visualization of 192-D speaker embeddings
- **Training History**: Loss and accuracy curves over epochs

## ğŸ“Š Results

### Performance Comparison

| Configuration              | Data Split         | Train Files | Test Files | Test EER  | Accuracy  | Validation EER |
| -------------------------- | ------------------ | ----------- | ---------- | --------- | --------- | -------------- |
| **Balanced** (Recommended) | 80/20 per speaker  | 13,725      | 3,605      | **7.88%** | **88.7%** | 4.41%          |
| Imbalanced Baseline        | Train/ folder only | 1,053       | 8,775      | 24.90%    | 62.5%     | 8.96%          |

**Improvement**: **68.4% relative improvement** in EER (24.90% â†’ 7.88%)

### Demo Testing Results

**Genuine Verification (Speaker 1034 vs 1034):**

- 10 test samples
- 10/10 correct acceptances (100% accuracy)
- Similarity scores: 0.5071 to 0.7101
- All above threshold (0.50)

**Impostor Detection (Speaker 1034 vs 1037):**

- 4 test samples
- 4/4 correct rejections (100% accuracy)
- Similarity scores: -0.0598 to 0.0589
- All below threshold (0.50)

### Output Files

Results saved in `results/` directory:

```
results/
â”œâ”€â”€ ecapa_results.txt               # Detailed metrics
â”œâ”€â”€ ecapa_roc_curve.png             # ROC curve (AUC visualization)
â”œâ”€â”€ ecapa_score_distribution.png    # Genuine vs impostor histograms
â””â”€â”€ ecapa_tsne.png                  # Embedding space visualization
```

### Key Findings

1. âœ… **Balanced data distribution is critical**: 68.4% improvement over imbalanced baseline
2. âœ… **Two-stage fine-tuning works well**: Frozen encoder prevents catastrophic forgetting
3. âœ… **ECAPA-TDNN generalizes to regional languages**: Despite training on English (VoxCeleb2)
4. âœ… **8 kHz sampling sufficient**: Telephone quality audio works for speaker verification
5. âœ… **Augmentation helps**: Speed/noise/reverb improves robustness

## ğŸ¤ Demo

### Interactive Verification

The `demo.py` script provides easy-to-use speaker verification:

**Single Verification:**

```bash
python demo.py single \
    --model checkpoints/ecapa_balanced/best_model.pt \
    --enroll data/Train/1034/1034_trn_vp_a_1.wav \
    --test data/Test/1034/1034_tst_vp_a_001.wav
```

**Batch Verification:**

```bash
python demo.py batch \
    --model checkpoints/ecapa_balanced/best_model.pt \
    --enroll-dir data/Train/1034 \
    --test-dir data/Test/1034
```

**Custom Threshold:**

```bash
python demo.py batch \
    --model checkpoints/ecapa_balanced/best_model.pt \
    --enroll-dir data/Train/1034 \
    --test-dir data/Test/1034 \
    --thresholds 0.3 0.5 0.7
```

### Demo Features

- âœ… Enrollment from multiple audio samples (average embedding)
- âœ… Batch testing against multiple files
- âœ… Multiple threshold evaluation
- âœ… Visual feedback (âœ“/âœ—) for each verification
- âœ… Accuracy reporting for genuine and impostor trials

## ğŸ”§ Troubleshooting

### Out of Memory Errors

```yaml
# In config file, reduce batch size
batch_size: 16 # or 8
```

### Slow Training

```yaml
# Increase DataLoader workers
num_workers: 4
pin_memory: true
```

### Poor Performance

- âœ… Ensure `use_combined_dataset: true` for balanced splitting
- âœ… Check `train_split: 0.8` for proper 80/20 ratio
- âœ… Verify audio files are 8 kHz mono WAV format
- âœ… Increase epochs if underfitting
- âœ… Enable data augmentation

### CUDA Out of Memory

```python
# Reduce batch size or use CPU
device = 'cpu'  # in config
```

## ğŸ“š References

### ECAPA-TDNN

```bibtex
@inproceedings{desplanques2020ecapa,
  title={ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification},
  author={Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
  booktitle={INTERSPEECH},
  year={2020}
}
```

### SpeechBrain

```bibtex
@misc{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Ravanelli, Mirco and Parcollet, Titouan and others},
  howpublished={\url{https://speechbrain.github.io/}},
  year={2021}
}
```

### AAM-Softmax Loss

```bibtex
@inproceedings{deng2019arcface,
  title={Arcface: Additive angular margin loss for deep face recognition},
  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  booktitle={CVPR},
  year={2019}
}
```

## ğŸ“ Citation

If you use this project in your research, please cite:

```bibtex
@misc{speaker_verification_regional,
  title={Speaker Verification System for Regional Languages (Hindi and Kannada)},
  author={Shrusti-04},
  year={2025},
  howpublished={\url{https://github.com/Shrusti-04/Speaker-Verification}}
}
```

## ğŸ“„ License

This project is for research and educational purposes.

## ğŸ‘¥ Contributors

- **Shrusti** - Implementation and Research

## ğŸ™ Acknowledgments

- SpeechBrain team for ECAPA-TDNN pretrained models
- VoxCeleb dataset creators for pretraining data
- Regional language dataset contributors (Hindi/Kannada)
- Google Colab for free GPU access (Tesla T4)

## ğŸ“§ Contact

For questions or issues:

- Open a [GitHub Issue](https://github.com/Shrusti-04/Speaker-Verification/issues)
- Repository: [https://github.com/Shrusti-04/Speaker-Verification](https://github.com/Shrusti-04/Speaker-Verification)

---

## ğŸŒŸ Highlights

- âœ… **7.88% EER** on Hindi/Kannada speaker verification
- âœ… **68.4% improvement** over imbalanced baseline
- âœ… **100% demo accuracy** on test samples
- âœ… **Production-ready** with interactive demo
- âœ… **Well-documented** with comprehensive guides
- âœ… **Balanced data strategy** for optimal performance

**This project demonstrates the critical importance of proper data distribution in speaker verification systems!**
