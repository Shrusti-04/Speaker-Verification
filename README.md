# Speaker Verification System for Regional Languages (Hindi & Kannada)

A deep learning-based speaker verification system implementing **ECAPA-TDNN** architecture for regional language (Hindi and Kannada) speaker recognition, achieving **7.88% EER** with balanced data distribution.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Achievements](#key-achievements)
- [Features](#features)
- [Getting Started](#getting-started)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Model Architecture](#model-architecture)
- [Training Strategy](#training-strategy)
- [Evaluation Metrics](#evaluation-metrics)
- [Results](#results)
- [Visualizations](#visualizations)
- [References](#references)

## ğŸ¯ Overview

This project implements a state-of-the-art speaker verification system designed for regional Indian languages, specifically Hindi and Kannada. The system uses:

- **ECAPA-TDNN Architecture**: Fine-tuned from VoxCeleb2 pretrained model
- **Balanced Data Distribution**: Per-speaker 80/20 train/test split
- **Two-Stage Fine-Tuning**: Frozen encoder followed by full training
- **Data Augmentation**: Speed perturbation, noise addition, and reverberation

## ğŸ† Key Achievements

- âœ… **7.88% Test EER** with balanced data distribution
- âœ… **88.7% Accuracy** on Hindi/Kannada speaker verification
- âœ… **68.4% Relative Improvement** over imbalanced baseline (24.90% â†’ 7.88%)
- âœ… **100% Demo Accuracy** on genuine verification (10/10) and impostor rejection (4/4)
- âœ… **351 Speakers** with 17,330 audio files
- âœ… **Two-Stage Fine-Tuning** with encoder freezing strategy

## âœ¨ Features

### Data Processing

- âœ… Balanced per-speaker 80/20 split (13,725 train / 3,605 test files)
- âœ… Automatic audio preprocessing (8kHz mono)
- âœ… Variable-length audio handling (2-10 seconds)
- âœ… Speaker-based data organization (351 speakers)

### Data Augmentation

- âœ… Speed perturbation (0.95x, 1.0x, 1.05x)
- âœ… Additive white noise (SNR 0-15 dB)
- âœ… Reverberation simulation

### Model Training

- âœ… Pretrained ECAPA-TDNN from VoxCeleb2
- âœ… Two-stage fine-tuning (frozen encoder â†’ full training)
- âœ… AAM-Softmax loss with margin=0.2, scale=30
- âœ… Adam optimizer with lr=0.0001
- âœ… Automatic checkpoint saving (best validation accuracy)
- âœ… Training history visualization

### Evaluation

- âœ… Equal Error Rate (EER) computation
- âœ… Accuracy metrics on test set
- âœ… ROC curves with visualization
- âœ… Score distribution plots (genuine vs impostor)
- âœ… t-SNE embedding space visualization
- âœ… Batch verification for multiple samples

## ğŸš€ Getting Started

### Prerequisites

- **Git LFS** (Large File Storage) - Required to download model checkpoints
- Python 3.8 or higher
- CUDA-capable GPU (recommended) or Google Colab

### Clone Repository

This repository uses **Git LFS** for storing large model checkpoint files (488 MB). You need to install Git LFS before cloning:

```bash
# Step 1: Install Git LFS (one-time setup)
# Windows: Download from https://git-lfs.github.com/
# Mac: brew install git-lfs
# Linux: sudo apt-get install git-lfs

# Step 2: Initialize Git LFS
git lfs install

# Step 3: Clone the repository (models will download automatically)
git clone https://github.com/Shrusti-04/Speaker-Verification.git
cd Speaker-Verification
```

**Note:** Without Git LFS, checkpoint files will be downloaded as small pointer files and the models won't work.

## ğŸ“ Project Structure

```
Speaker-Verification/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ecapa_balanced_config.yaml  # Balanced split configuration (7.88% EER)
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ dataset.py                  # Dataset with balanced splitting
â”‚   â”œâ”€â”€ augmentation.py             # Audio augmentation
â”‚   â”œâ”€â”€ evaluation.py               # EER and metrics computation
â”‚   â”œâ”€â”€ verification.py             # Cosine similarity verification
â”‚   â”œâ”€â”€ visualization.py            # Plotting utilities
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ ecapa_tdnn.py          # ECAPA-TDNN wrapper + AAMSoftmax
â”œâ”€â”€ data/                           # Dataset
â”‚   â”œâ”€â”€ Train/                     # 351 speakers with audio files
â”‚   â””â”€â”€ Test/                      # 351 speakers with audio files
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ ecapa_balanced/            # Best model (7.88% EER)
â”œâ”€â”€ results/                        # Evaluation outputs
â”‚   â”œâ”€â”€ ecapa_results.txt          # Metrics
â”‚   â”œâ”€â”€ ecapa_roc_curve.png        # ROC curve
â”‚   â”œâ”€â”€ ecapa_score_distribution.png
â”‚   â””â”€â”€ ecapa_tsne.png
â”œâ”€â”€ paper/                          # Documentation
â”‚   â””â”€â”€ figures/                   # Result visualizations
â”œâ”€â”€ train.py                        # Main training script
â”œâ”€â”€ evaluate.py                     # Evaluation script
â””â”€â”€ demo.py                         # Interactive verification demo
```

## ğŸ“Š Dataset

### Dataset Characteristics

- **Languages**: Hindi and Kannada
- **Total Speakers**: 351
- **Total Audio Files**: 17,330
- **Files per Speaker**: ~49 (average)
- **Balanced Split**: 80% train (13,725 files) / 20% test (3,605 files) per speaker
- **Audio Format**:
  - Sample rate: 8 kHz (telephone quality)
  - Duration: Variable (~2-10 seconds)
  - Channels: Mono
  - Bit depth: 16-bit
  - Format: WAV

### Dataset Structure

```
data/
â”œâ”€â”€ Train/
â”‚   â”œâ”€â”€ 1034/
â”‚   â”‚   â”œâ”€â”€ 1034_trn_vp_a_1.wav
â”‚   â”‚   â””â”€â”€ ... (~49 files)
â”‚   â””â”€â”€ ... (351 speakers)
â””â”€â”€ Test/
    â”œâ”€â”€ 1034/
    â”‚   â”œâ”€â”€ 1034_tst_vp_a_001.wav
    â”‚   â””â”€â”€ ... (~49 files)
    â””â”€â”€ ... (351 speakers)
```

**Note**: Both Train/ and Test/ folders are combined and re-split 80/20 per speaker for balanced distribution.

## ğŸ—ï¸ Model Architecture

### ECAPA-TDNN (Emphasized Channel Attention, Propagation and Aggregation in TDNN)

The system uses the **ECAPA-TDNN** architecture, a state-of-the-art speaker embedding model that significantly improves upon the original x-vector TDNN architecture.

#### Key Components

**1. Frame-level Feature Extraction**

- **Input**: 80-dimensional Mel-filterbank features (8 kHz sampling)
- **Conv1D Layer**: Initial 1D convolutional layer (512 channels, kernel size=5)
- **SE-Res2Blocks**: 3 Squeeze-and-Excitation Res2Net blocks with channel attention
  - Scale-dimension: 8 (multi-scale receptive fields)
  - Dilation rates: 2, 3, 4 (increasing temporal context)
  - SE attention: Recalibrates channel-wise features

**2. Channel Attention Mechanism**

- **Squeeze-and-Excitation (SE)**: Enhances important acoustic features
- **Multi-scale Processing**: Res2Net blocks capture diverse temporal patterns
- **Channel Propagation**: Information flow across feature channels

**3. Statistical Pooling**

- **Attentive Statistics Pooling (ASP)**: Learned attention mechanism
- Aggregates variable-length audio into fixed-size embeddings
- Combines mean and standard deviation statistics
- Weighted by attention scores for relevant frames

**4. Embedding Layer**

- **Fully Connected Layers**: 1536 â†’ 192 dimensions
- **Batch Normalization**: Stabilizes training
- **Output**: 192-dimensional speaker embeddings
- **L2 Normalization**: Unit-length embeddings for cosine similarity

**5. Classification Head (Training Only)**

- **AAM-Softmax Loss**: Additive Angular Margin Softmax
  - Margin: 0.2 (angular separation between speakers)
  - Scale: 30 (controls gradient flow)
  - Creates discriminative decision boundaries
- **Linear Layer**: 192 â†’ 351 speakers

#### Architecture Advantages

âœ… **Channel Attention**: Emphasizes discriminative acoustic features  
âœ… **Multi-scale Processing**: Captures both short-term and long-term patterns  
âœ… **Efficient Embeddings**: Compact 192-D representation  
âœ… **Transfer Learning**: Pretrained on VoxCeleb2 (1M+ utterances)  
âœ… **Robustness**: Attentive pooling handles variable-length audio

#### Model Parameters

- **Embedding Dimension**: 192
- **Channels**: 512, 512, 512, 1536, 192
- **SE-Res2Block Scales**: 8
- **Total Parameters**: ~6.3M
- **Input**: 80-dim MFBs at 8 kHz
- **Output**: 192-dim embeddings

#### Pretrained Weights

The model is initialized with weights pretrained on **VoxCeleb2**:

- Dataset: 1M+ utterances from 6K+ speakers
- Language: English (cross-lingual transfer to Hindi/Kannada)
- Training: Large-scale speaker recognition on diverse audio

## ğŸ”¬ Training Strategy

### Two-Stage Fine-Tuning

**Stage 1: Frozen Encoder (Epochs 1-5)**

- Encoder weights frozen (pretrained from VoxCeleb2)
- Only classifier layer trains
- Adapts to 351 speakers
- Faster convergence

**Stage 2: Full Training (Epochs 6-15)**

- All layers unfrozen
- End-to-end fine-tuning
- Adapts to Hindi/Kannada audio characteristics
- Achieves final performance

### Balanced Data Distribution

- Combines Train/ and Test/ folders
- Splits 80/20 per speaker
- Result: 13,725 train / 3,605 test files
- **Achieved 7.88% EER** (68.4% improvement over imbalanced baseline)

### Data Augmentation

Applied randomly during training:

- **Speed Perturbation**: 0.95x, 1.0x, 1.05x (33% each)
- **Additive Noise**: White noise, SNR 0-15 dB
- **Reverberation**: Simulates room acoustics

### Hyperparameters

- **Optimizer**: Adam
- **Learning Rate**: 0.0001 (constant)
- **Batch Size**: 32
- **Max Epochs**: 15
- **Loss Function**: AAM-Softmax (margin=0.2, scale=30)
- **Hardware**: Google Colab Tesla T4 GPU
- **Training Time**: ~4-5 hours

## ğŸ“ˆ Evaluation Metrics

### Equal Error Rate (EER)

The threshold where False Acceptance Rate (FAR) equals False Rejection Rate (FRR).

- **Lower is better**
- Our Result: **7.88% EER**

### Accuracy

Percentage of correct verification decisions (genuine acceptance + impostor rejection).

- Our Result: **88.7% accuracy**

### Cosine Similarity Scoring

Measures similarity between speaker embeddings:

- Range: [-1, 1]
- Higher values indicate same speaker
- Threshold: 0.50 (optimized for EER)

### Visualization Outputs

- **ROC Curves**: True Positive Rate vs False Positive Rate
- **Score Distributions**: Genuine vs impostor score histograms
- **t-SNE Plots**: 2D visualization of 192-D speaker embeddings
- **Training History**: Loss and accuracy curves over epochs

## ğŸ“Š Results

### Performance Metrics (Balanced Implementation)

| Metric                        | Value     |
| ----------------------------- | --------- |
| **Test EER**                  | **7.88%** |
| **Test Accuracy**             | **88.7%** |
| **Validation EER**            | 4.41%     |
| **Training Files**            | 13,725    |
| **Test Files**                | 3,605     |
| **Improvement over Baseline** | **68.4%** |

### Demo Testing Results

**Genuine Verification (Speaker 1034 vs 1034):**

- 10/10 correct acceptances (100% accuracy)
- Similarity scores: 0.5071 to 0.7101

**Impostor Detection (Speaker 1034 vs 1037):**

- 4/4 correct rejections (100% accuracy)
- Similarity scores: -0.0598 to 0.0589

### Key Findings

1. âœ… **Balanced data distribution is critical**: 68.4% improvement over imbalanced baseline
2. âœ… **Two-stage fine-tuning works well**: Frozen encoder prevents catastrophic forgetting
3. âœ… **ECAPA-TDNN generalizes to regional languages**: Despite training on English (VoxCeleb2)
4. âœ… **8 kHz sampling sufficient**: Telephone quality audio works for speaker verification
5. âœ… **Augmentation helps**: Speed/noise/reverb improves robustness

## ğŸ“Š Visualizations

### Training History

![Training History](logs/ecapa_balanced/training_history.png)

_Training and validation loss/accuracy curves over 15 epochs showing two-stage fine-tuning (encoder frozen until epoch 5, then fully trained)_

### ROC Curve

![ROC Curve](results/ecapa_roc_curve.png)

_Receiver Operating Characteristic curve showing the trade-off between True Positive Rate and False Positive Rate at different thresholds_

### DET Curve

![DET Curve](results/ecapa_det_curve.png)

_Detection Error Tradeoff curve showing False Rejection Rate vs False Acceptance Rate on logarithmic scale, alternative view optimized for low error rates_

### Score Distribution

![Score Distribution](results/ecapa_score_distribution.png)

_Distribution of cosine similarity scores for genuine (same speaker) and impostor (different speaker) pairs, demonstrating clear separation_

### t-SNE Embedding Visualization

![t-SNE Visualization](results/ecapa_tsne.png)

_2D visualization of 192-dimensional speaker embeddings using t-SNE, showing distinct clustering of different speakers_

### PCA Embedding Visualization

![PCA Visualization](results/ecapa_pca.png)

_2D PCA projection of speaker embeddings showing the first two principal components with explained variance, demonstrating linear separability_

## ğŸŒŸ Project Highlights

- âœ… **7.88% EER** on Hindi/Kannada speaker verification
- âœ… **68.4% improvement** through balanced data distribution
- âœ… **100% demo accuracy** on test samples
- âœ… **Two-stage fine-tuning** strategy for optimal performance
- âœ… **Production-ready** implementation with comprehensive evaluation

**This project demonstrates the critical importance of proper data distribution in speaker verification systems for regional languages!**
